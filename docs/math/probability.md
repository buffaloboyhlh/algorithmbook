# 概率统计

## 一、概率统计面试题与答案

### 1.1 热门面试题

#### 条件概率

设事件 A 和 B，且 $P(B) > 0$，则在事件 B 已发生的条件下，事件 A 发生的条件概率为：

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

其中：

- $P(A|B)$：在事件 B 发生的前提下，事件 A 发生的概率。
- $P(A \cap B)$：事件 A 和事件 B 同时发生的概率。
- $P(B)$：事件 B 发生的概率。

该公式表明，条件概率是通过将两个事件的联合概率除以条件事件的概率来计算的。

#### 全概率公式

设试验的样本空间为 $S$，事件 $B_1, B_2, \ldots, B_n$ 构成 $S$ 的一个划分，即：

- $B_i$ 两两互斥（$B_i \cap B_j = \emptyset$，当 $i \neq j$）；
- 它们的并集为整个样本空间（$\bigcup_{i=1}^n B_i = S$）；
- 每个 $B_i$ 的概率大于零（$P(B_i) > 0$）。
对于任意事件 $A$，其概率可表示为：

$$
P(A) = \sum_{i=1}^n P(B_i) \cdot P(A | B_i)
$$

步骤详解

1. 确定完备事件组：找到一组事件 $B_1, B_2, \ldots, B_n$，它们互斥且覆盖所有可能性。
2. 计算先验概率：求出每个 $B_i$ 的概率 $P(B_i)$。
3. 计算条件概率：求出在每个 $B_i$ 发生的条件下，事件 $A$ 发生的概率 $P(A | B_i)$。
4. 应用全概率公式：将上述结果代入公式，求和得到 $P(A)$。

示例

例：某人从甲箱（4红2白）随机取一球放入乙箱（3红3白），再从乙箱取一球。求从乙箱取出红球的概率。

- 步骤：

  - 设 $B_1$ 为从甲箱取红球，$B_2$ 为取白球。
  - $P(B_1) = \frac{4}{6}$，$P(B_2) = \frac{2}{6}$。
  - $P(A | B_1) = \frac{4}{7}$，$P(A | B_2) = \frac{3}{7}$。
  - $P(A) = \frac{4}{6} \cdot \frac{4}{7} + \frac{2}{6} \cdot \frac{3}{7} = \frac{22}{42} = \frac{11}{21}$。

应用场景

- 医学诊断：评估患者患病概率，考虑年龄、性别、生活习惯等因素。
- 投资决策：分析市场走势、政策变化对投资收益的影响。
- 机器学习：计算样本属于某类别的概率，基于特征空间划分和条件概率估计。


#### 贝叶斯定理

贝叶斯定理是概率论中的一个重要概念，用于计算在已知某些条件下，事件发生的概率。以下是贝叶斯定理的详细教程：

贝叶斯定理的公式为：

$$
P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
$$

- P(A)：事件A的先验概率，即在考虑新证据前对A发生概率的估计。
- P(B|A)：在A发生的条件下，B发生的概率。
- P(B)：事件B的总概率。
- P(A|B)：在B发生的条件下，A发生的后验概率，即考虑新证据后对A发生概率的更新。

贝叶斯定理的核心思想

1. 逆向推理：从结果反推原因，而非传统的“原因→结果”逻辑。
2. 动态更新：随着新证据的出现，不断修正概率，使判断更加准确。
3. 比例思维：关注“目标原因”占“所有可能性”的比例。


**示例**

贝叶斯定理就是“先有点预判，然后拿到新证据，再更新你的判断”。

生活中的例子：感冒 vs. 打喷嚏

- 先验概率：冬天里一般有10%的人感冒，即 $P(感冒) = 0.1$。
- 似然概率：

    - 感冒的人有80%的概率会打喷嚏，即 $P(打喷嚏|感冒) = 0.8$。
    - 健康的人有5%的概率打喷嚏，即 $P(打喷嚏|不感冒) = 0.05$。
  
- 后验概率：看到一个人打喷嚏，他是感冒的概率是多少？
    - $P(感冒|打喷嚏) = \frac{P(打喷嚏|感冒) \cdot P(感冒)}{P(打喷嚏)}$
    - $P(打喷嚏) = P(打喷嚏|感冒) \cdot P(感冒) + P(打喷嚏|不感冒) \cdot P(不感冒) = 0.8 \cdot 0.1 + 0.05 \cdot 0.9 = 0.08 + 0.045 = 0.125$
    - $P(感冒|打喷嚏) = \frac{0.8 \cdot 0.1}{0.125} = \frac{0.08}{0.125} = 0.64$

结论：看到他打喷嚏后，他感冒的概率从10%提高到了64%

#### 最大似然估计（MLE）

最大似然估计就是“假设你不知道真实情况，但你有一堆数据，你想找到最可能的解释”。
生活中的例子：猜测朋友的口味

- 观察数据：朋友10天的点单记录：8次半糖，2次少糖，从未点无糖或全糖。
- 最可能的解释：他最喜欢的甜度应该是“半糖”（因为这是他点得最多的）。
- 数学解释：找到使得观察到的数据最可能出现的参数。

贝叶斯定理 vs. MLE

| 对比点 | 贝叶斯定理 | 最大似然估计（MLE） |
| --- | --- | --- |
| 主要思想 | 用新证据更新旧判断 | 找到最可能的解释 |
| 需要先验知识 | 要有先验概率 | 不需要先验知识 |
| 适用场景 | 医学诊断、机器学习、垃圾邮件检测 | 统计推断、机器学习、参数估计 |

总结

- 贝叶斯定理：适合处理不确定性问题，通过先验概率和新证据更新判断。
- 最大似然估计：适合参数估计，通过数据找到最可能的参数。

#### 二项分布和柏松分布

**二项分布**

1. 定义与特点
    - 二项分布描述了在n次独立的伯努利试验中，事件成功次数的概率分布。每次试验只有两种可能结果：成功（概率为p）或失败（概率为1-p）。
    - 记作$X \sim B(n, p)$，其中n为试验次数，p为单次试验成功的概率。
2. 概率质量函数
    - $P(X = k) = C(n, k) \cdot p^k \cdot (1-p)^{n-k}$，其中$C(n, k)$为组合数，表示从n次试验中选择k次成功的方式数。
3. 期望与方差
    - 期望：$E(X) = np$。
    - 方差：$Var(X) = np(1-p)$。
4. 应用场景
    - 适用于试验次数固定、每次试验独立且成功概率相同的情况，如抛硬币、产品质量抽检等。

**泊松分布**

1. 定义与特点
    - 泊松分布描述了在固定时间或空间内，随机事件发生次数的概率分布。适用于事件发生概率小、但试验次数大的情况。
    - 记作$X \sim P(\lambda)$，其中$\lambda$为单位时间（或单位空间）内事件的平均发生率。
2. 概率质量函数
    - $P(X = k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda}$，其中k为非负整数。
3. 期望与方差
    - 期望和方差均为$\lambda$。
4. 应用场景
    - 适用于稀有事件的计数，如电话呼叫次数、交通事故发生次数、放射性粒子的衰变等。

**二项分布与泊松分布的关系**

- 泊松近似
    - 当二项分布的试验次数n很大，成功概率p很小，且$np = \lambda$为适中值时，二项分布可用泊松分布近似，即$B(n, p) \approx P(\lambda)$。
    - 这种近似简化了计算，特别适用于n较大时的二项分布计算。
- 极限关系
    - 泊松分布是二项分布的极限分布，当$n \rightarrow \infty$且$p \rightarrow 0$，满足$np \rightarrow \lambda$时，二项分布趋近于泊松分布。



