# 机器学习算法

## 一、线性回归

### 1.什么是线性回归

- 线性：两个变量之间的关系**是**一次函数关系的——图象**是直线**，叫做线性。
- 非线性：两个变量之间的关系**不是**一次函数关系的——图象**不是直线**，叫做非线性。
- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。


| 特性 | 一元线性回归 | 多元线性回归 | 多项式回归 |
|------|----------------|----------------|----------------|
| 自变量数量 | 1个 | 2个或以上 | 1个（含高次项） |
| 模型形式 | 线性直线 | 多维超平面 | 曲线（非线性拟合） |
| 适用场景 | 单因素影响分析 | 多因素综合影响研究 | 非线性关系建模 |
| 关键假设 | 线性、正态误差、等方差、独立性 | 同左，外加无多重共线性 | 同线性回归，但放宽线性假设 |
| 参数估计 | 最小二乘法 | 最小二乘法或正则化方法 | 最小二乘法结合模型选择 |



### 2. 能够解决什么样的问题

对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。


### 3. 一般表达式是什么

![](https://latex.codecogs.com/gif.latex?Y=wx+b)

w叫做x的系数，b叫做偏置项。

### 4. 如何计算

#### 4.1 Loss Function--MSE

![](https://latex.codecogs.com/gif.latex?J=\frac{1}{2m}\sum^{i=1}_{m}(y^{'}-y)^2)

利用**梯度下降法**找到最小值点，也就是最小误差，最后把 w 和 b 给求出来。

### 5. 过拟合、欠拟合如何解决

使用正则化项，也就是给loss function加上一个参数项，正则化项有**L1正则化、L2正则化、ElasticNet**。加入这个正则化项好处：

- 控制参数幅度，不让模型“无法无天”。
- 限制参数搜索空间
- 解决欠拟合与过拟合的问题。

#### 5.1 什么是L2正则化(岭回归)

方程：

![](https://latex.codecogs.com/gif.latex?J=J_0+\lambda\sum_{w}w^2)

![](https://latex.codecogs.com/gif.latex?J_0)表示上面的 loss function ，在loss function的基础上加入w参数的平方和乘以 ![](https://latex.codecogs.com/gif.latex?\lambda) ，假设：

![](https://latex.codecogs.com/gif.latex?L=\lambda({w_1}^2+{w_2}^2))

回忆以前学过的单位元的方程：

![](https://latex.codecogs.com/gif.latex?x^2+y^2=1)

正和L2正则化项一样，此时我们的任务变成在L约束下求出J取最小值的解。求解J0的过程可以画出等值线。同时L2正则化的函数L也可以在w1 w2的二维平面上画出来。如下图：

![image](https://camo.githubusercontent.com/3886773758620787d7ef9b904e4ff629371dce967c12dfe672d4a77e2fefc2fb/68747470733a2f2f7778342e73696e61696d672e636e2f6c617267652f303036333044656667793167346e7339716861316e6a333038753038396161762e6a7067)

L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得w1和w2无限接近于0，达到防止过拟合的问题。


#### 5.2 什么场景下用L2正则化

只要数据线性相关，用LinearRegression拟合的不是很好，**需要正则化**，可以考虑使用岭回归(L2), 如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。

#### 5.3 什么是L1正则化(Lasso回归)

L1正则化与L2正则化的区别在于惩罚项的不同：

![](https://latex.codecogs.com/gif.latex?J=J_0+\lambda(|w_1|+|w_2|))

求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图：

![image](https://i-blog.csdnimg.cn/blog_migrate/1e9fc3e092f6f3cb9ddfa376f869f464.png)

惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。**这就说明了L1正则化容易得到稀疏矩阵。**

#### 5.4 什么场景下使用L1正则化

**L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0**，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。

#### 5.5 什么是ElasticNet回归

**ElasticNet综合了L1正则化项和L2正则化项**，以下是它的公式：

![](https://latex.codecogs.com/gif.latex?min(\frac{1}{2m}[\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]+\lambda\sum_{j=1}^{n}|\theta|))

#### 5.6  ElasticNet回归的使用场景

ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。

### 6. 线性回归要求因变量服从正态分布？

我们假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布$N(0,delta^2)$时，因变量则符合正态分布$N(ax(i)+b,delta^2)$，其中预测函数$y=ax(i)+b$。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 

在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。

### 7. 模型评估指标

#### 7.1 均方误差（MSE, Mean Squared Error）

均方误差是预测值与真实值之差的平方的平均值。MSE越小，表示模型的预测效果越好。

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

其中，$y_i$ 是真实值，$\hat{y}_i$ 是预测值，$n$ 是样本数量。

```python
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_true, y_pred)
```


#### 7.2 均方根误差（RMSE, Root Mean Squared Error）

均方根误差是均方误差的平方根，具有与原始数据相同的单位，更易于解释。

$$
RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^
2}
$$

```python
mean_squared_error(y_real, y_predict, squared=False) # squared=False 表示返回的是 RMSE 而不是 MSE。
```


#### 7.3 平均绝对误差（MAE, Mean Absolute Error）

平均绝对误差是预测值与真实值之差的绝对值的平均值。MAE越小，表示模型的预测效果越好。

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

```python
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)
```

#### 7.4 R²（决定系数, Coefficient of Determination）

R²衡量模型解释变量总变异的比例，取值范围为0到1。R²越接近1，表示模型的解释能力越强。越接近1表示模型拟合越好

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

其中，$\bar{y}$ 是真实值的均值。

```python
from sklearn.metrics import r2_score
r2 = r2_score(y_true, y_pred) 
```



### 8. 代码实现

##### 题目描述

从给定的房屋基本信息以及房屋销售信息等，建立一个回归模型预测房屋的销售价格。 数据下载请点击：[下载](https://pan.baidu.com/share/init?surl=kVdwI3d)，密码：mfqy。

**数据说明：** 数据主要包括2014年5月至2015年5月美国King County的房屋销售价格以及房屋的基本信息。 数据分为训练数据和测试数据，分别保存在kc_train.csv和kc_test.csv两个文件中。 其中训练数据主要包括10000条记录，14个字段，主要字段说明如下： 第一列“销售日期”：2014年5月到2015年5月房屋出售时的日期 第二列“销售价格”：房屋交易价格，单位为美元，是目标预测值 第三列“卧室数”：房屋中的卧室数目 第四列“浴室数”：房屋中的浴室数目 第五列“房屋面积”：房屋里的生活面积 第六列“停车面积”：停车坪的面积 第七列“楼层数”：房屋的楼层数 第八列“房屋评分”：King County房屋评分系统对房屋的总体评分 第九列“建筑面积”：除了地下室之外的房屋建筑面积 第十列“地下室面积”：地下室的面积 第十一列“建筑年份”：房屋建成的年份 第十二列“修复年份”：房屋上次修复的年份 第十三列"纬度"：房屋所在纬度 第十四列“经度”：房屋所在经度

#### 1. 读取数据与预处理

```python
import numpy as np
import pandas as pd

column_names = ["销售日期", "销售价格", "卧室数", "浴室数",
                "房屋面积", "停车面积", "楼层数", "房屋评分",
                "建筑面积", "地下室面积", "建筑年份", "修复年份",
                "纬度", "经度"]

# 读取数据
train_data = pd.read_csv("kc_train.csv", names=column_names)
# 数据预处理
train_data.info()  #查看是否有缺失值

# 选取特征
X = train_data[["卧室数", "浴室数",
                "房屋面积", "停车面积", "楼层数", "房屋评分",
                "建筑面积", "地下室面积", "建筑年份", "修复年份"]]
# 选取目标
Y = train_data["销售价格"]

print(X.shape)
print(Y.shape)
```

#### 2. 特征选择

```python
# 统一维度
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
```

#### 3. 模型训练

```python
# 模型训练
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
model = LinearRegression()
model.fit(X, Y) #模型训练

preds = model.predict(X)
mse = mean_squared_error(Y, preds)
print("训练RMSE：",np.sqrt(mse))
print("线性模型参数 W：",model.coef_)
print("线性模型截距：",model.intercept_)
```

#### 4. 模型评估

```python
# 绘图进行比较
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = ['Songti SC']
num = 100
x = np.arange(num)

plt.plot(x,preds[:num],label="预测值")
plt.plot(x,Y[:num],label="真实值")
plt.legend()
plt.show()
```

## 二、逻辑回归


### 1. 什么是逻辑回归

逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞, +∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。

也就是把Y的结果带入一个非线性变换的**Sigmoid函数**中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。

### 2. 什么是Sigmoid函数

函数公式如下：

![image](https://pic3.zhimg.com/v2-495e8dd0294d5986e400c866b34b4ac4_r.jpg)

函数中t无论取什么值，其结果都在[0,1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的**阈值**是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。

好了，接下来我们把aX+b带入t中就得到了我们的逻辑回归的一般模型方程：

![](https://latex.codecogs.com/gif.latex?H(a,b)=\frac{1}{1+e^{(aX+b)}})

结果P也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。

### 3. 损失函数是什么

逻辑回归常用交叉熵损失函数（Cross-Entropy Loss），也叫对数损失（Log Loss）。

#### 二分类问题的交叉熵损失函数

对于二分类问题，假设样本的真实标签为 $y_i \in \{0, 1\}$，模型的预测概率为 $\hat{y}_i = \sigma(z_i)$，其中 $z_i = \mathbf{w}^T \mathbf{x}_i + b$ 是线性组合的输出，$\sigma$ 为 Sigmoid 函数，$\mathbf{w}$ 和 $b$ 分别是模型的权重向量和偏置项。
交叉熵损失函数定义为：

$$
L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

其中，$N$ 是样本数量。

**损失函数的解释**

- 当 $y_i = 1$ 时：损失函数为 $-\log(\hat{y}_i)$，预测概率 $\hat{y}_i$ 越接近 1，损失越小。
- 当 $y_i = 0$ 时：损失函数为 $-\log(1 - \hat{y}_i)$，预测概率 $\hat{y}_i$ 越接近 0，损失越小。

**损失函数的性质**

- 非负性：损失函数的值总是非负的。
- 凸性：对于逻辑回归，损失函数是凸函数，可以通过梯度下降等优化方法找到全局最优解。
- 概率解释：损失函数本质上是最大化样本标签的预测概率。

**梯度下降优化**
为了最小化损失函数，通常使用梯度下降算法来更新参数 $\mathbf{w}$ 和 $b$。

- 权重 $\mathbf{w}$ 的梯度：

$$
  \frac{\partial L}{\partial \mathbf{w}} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) \mathbf{x}_i
$$

- 偏置 $b$ 的梯度：

$$
  \frac{\partial L}{\partial b} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)
$$

**梯度下降更新规则**

- 权重更新：

$$
  \mathbf{w} := \mathbf{w} - \alpha \cdot \frac{\partial L}{\partial \mathbf{w}}
$$

- 偏置更新：

$$
  b := b - \alpha \cdot \frac{\partial L}{\partial b}
$$

其中，$\alpha$ 是学习率。

**总结**

交叉熵损失函数在逻辑回归中用于衡量模型预测概率与真实标签之间的差异，通过梯度下降算法优化模型参数，使得预测概率尽可能接近真实标签，从而提高分类的准确性。

[交叉损失熵](https://zhuanlan.zhihu.com/p/638725320)


### 4.可以进行多分类吗？

可以的，其实我们可以从二分类问题过度到多分类问题(one vs rest)，思路步骤如下：

1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。

2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。

3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率pi，最后我们取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。

![image](https://wx2.sinaimg.cn/large/00630Defly1g4pw11fo1tj30cv0c50tj.jpg)

总之还是以二分类来依次划分，并求出最大概率结果。

### 5.逻辑回归有什么优点

- LR能以概率的形式输出结果，而非只是0,1判定。
- LR的可解释性强，可控度高(你要给老板讲的嘛…)。
- 训练快，feature engineering之后效果赞。
- 因为结果是概率，可以做ranking model。

### 6. 逻辑回归有哪些应用

- CTR预估/推荐系统的learning to rank/各种分类场景。
- 某搜索引擎厂的广告CTR预估基线版是LR。
- 某电商搜索排序/广告CTR预估基线版是LR。
- 某电商的购物搭配推荐用了大量LR。
- 某现在一天广告赚1000w+的新闻app排序基线是LR。

### 7. 逻辑回归常用的优化方法有哪些

#### 7.1 一阶方法

梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 

#### 7.2 二阶方法：牛顿法、拟牛顿法： 

这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。

拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

### 8. 逻辑斯特回归为什么要对特征进行离散化。

1. 非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
2. 速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
3. 鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 
4. 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 
5. 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 
6. 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 9. 逻辑回归的目标函数中增大L1正则化会是什么结果。

所有的参数w都会变成0。

### 10. 代码实现

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 加载数据集
data = load_iris()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 数据标准化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 训练模型
model = LogisticRegression(multi_class='ovr')
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 评估
print(classification_report(y_test, y_pred))
```


## 三、决策树

### 3.1 决策树的基本概念

决策树是机器学习中常用的监督学习算法，用于分类和回归任务。它通过树状结构模拟决策过程，每个内部节点代表一个特征上的判断条件，每个分支代表判断结果，叶节点代表最终的预测结果。

####  基本结构
- 根节点：树的起点，包含所有样本数据，是第一个特征判断的起点。
- 内部节点：表示对某个特征的判断条件，每个内部节点会根据特征值分裂为多个子节点。
- 分支：连接节点的线段，代表特征判断的结果。
- 叶节点：树的终点，输出最终的预测结果。

####  核心原理
- 特征选择：选择最优特征进行节点分裂，常用指标包括信息增益、信息增益率和基尼指数。
    - 信息熵：衡量数据集的纯度，熵值越低，纯度越高。
    - 信息增益：表示特征使得类不确定性的减少程度，增益越大，特征越优。
    - 信息增益率：对信息增益进行归一化，抑制对多取值属性的偏好。
    - 基尼指数：衡量数据集的不纯度，基尼指数越小，纯度越高。
- 树的生成：递归选择最优特征，分割训练数据，生成决策树。
- 树的剪枝：简化决策树，提高泛化能力，包括预剪枝和后剪枝。

####  典型算法
- ID3算法：基于信息增益选择特征，生成决策树。
- C4.5算法：改进ID3，使用信息增益率选择特征，可处理连续值和缺失值。
- CART算法：分类与回归树，可处理分类和回归任务，使用基尼指数或平方误差选择特征。

####  优缺点
- 优点
    - 易于理解和解释：树形结构直观，规则清晰。
    - 处理多种数据类型：可处理数值型和类别型数据。
    - 不需要数据预处理：对缺失值不敏感，无需归一化。
- 缺点
    - 容易过拟合：需通过剪枝提高泛化能力。
    - 不稳定：数据微小变化可能导致树结构变化较大。

####  应用场景
- 分类任务：如客户分类、疾病诊断。
- 回归任务：如房价预测、销售额预测。
- 特征选择：通过决策树选择重要特征。

决策树因其直观性和可解释性，成为机器学习中的基础算法，为更复杂的集成学习算法（如随机森林、GBDT）奠定了基础。

### 3.2 重要概念

#### 熵（Entropy）

衡量数据集的纯度或不确定性。熵越低，数据越纯。

$$
H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
$$

数据集 S 有 c 类，第 i 类样本比例为 $p_i$。

#### 信息增益（Information Gain）

**信息增益：** 以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。

信息增益 = entroy(前) - entroy(后)

$$
IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)
$$

数据集 S 按特征 A 的取值划分为子集 $S_v$。

#### 信息增益率（Information Gain Ratio）

信息增益率是信息增益的归一化形式，解决了信息增益对多值特征的偏好问题。


$$
IGR(S, A) = \frac{IG(S, A)}{H_A(S)}
$$

其中，$H_A(S)$ 是特征 A 的固有熵：

$$
H_A(S) = -\sum_{v \in Values(A)} \frac{|S v|}{|S|} \log_2 \frac{|S_v|}{|S|}
$$

#### 基尼指数（Gini Index）

基尼指数衡量数据集的不纯度，值越小表示数据越纯。

$$
Gini(S) = 1 - \sum_{i=1}^{c} p_i^2 
$$

数据集 S 有 c 类，第 i 类样本比例为 $p_i$。

### 3.3 决策树的代码实现

```pythond

```













