# 机器学习中的数学

## 1. 最小二乘法

最小二乘法（Least Squares Method）是一种用于曲线拟合和参数估计的数学优化技术，其核心思想是通过最小化误差的平方和来寻找数据的最佳函数匹配。以下是其详细解析：

###  基本原理
- 给定一组数据点 $(x_i, y_i)$（$i=1,2,...,m$），假设其关系可由函数 $f(x, w)$ 描述（$w$ 为待定参数），目标是找到 $w$ 使得函数输出与真实数据的误差平方和最小。
- 误差函数定义为：$\delta = \sum_{i=1}^{m} (f(x_i, w) - y_i)^2$。当 $\delta$ 最小时，对应的 $w$ 即为最优参数。

###  线性最小二乘法
- 当拟合函数为线性组合时（如 $f(x) = w_0 + w_1 x$），可通过求解正规方程组得到解析解。
- 以一元线性回归为例，需最小化：$\varphi = \sum_{i=1}^{m} (a_0 + a_1 x_i - y_i)^2$。
- 对参数 $a_0$、$a_1$ 求偏导并令其为0，得到方程组：
  $ma_0 + (\sum x_i) a_1 = \sum y_i$
  $(\sum x_i) a_0 + (\sum x_i^2) a_1 = \sum (x_i y_i)$。
- 解此方程组可得参数 $a_0$、$a_1$，从而得到最佳拟合直线。

###  非线性最小二乘法
- 当拟合函数非线性时，通常无解析解，需通过迭代算法（如梯度下降、高斯-牛顿法）逐步优化参数。

###  应用与实现
- 曲线拟合：最小二乘法可拟合数据趋势，预测未知值。
- 参数估计：在回归分析中，普通最小二乘法（OLS）通过最小化残差平方和估计模型参数。
- 代码实现：Python中可使用`statsmodels`或`numpy`库实现：

  ```python
  import statsmodels.api as sm
  model = sm.OLS(y, X).fit()
  print(model.summary())
  ```

最小二乘法因其原理简单、计算高效，成为数据分析和机器学习的基础工具之一。


## 2. 全概率公式

全概率公式是概率论中用于计算复杂事件概率的重要工具。它通过将复杂事件分解为若干互斥且完备的简单事件，分别计算各简单事件的概率及其对复杂事件的影响，再求和得到最终结果。

####  公式表达
设事件$B_1, B_2, \ldots, B_n$构成样本空间的一个划分（即互斥且并集为全集），且$P(B_i) > 0$，则对任意事件$A$，有：

$$
P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A|B_i)
$$

其中：

- $P(B_i)$：事件$B_i$发生的概率。
- $P(A|B_i)$：在$B_i$发生的条件下，事件$A$发生的概率。

####  关键概念
- 样本空间的划分：将样本空间划分为互斥且完备的事件组，确保所有可能性都被覆盖。
- 先验概率：$P(B_i)$表示在考虑条件前的初始概率。
- 条件概率：$P(A|B_i)$表示在特定条件下事件$A$的概率。
####  应用步骤
1. 确定划分：找出导致事件$A$发生的所有互斥且完备的原因（$B_1, B_2, \ldots, B_n$）。
2. 计算先验概率：计算每个原因$B_i$发生的概率$P(B_i)$。
3. 计算条件概率：确定在每个原因$B_i$下，事件$A$发生的概率$P(A|B_i)$。
4. 应用公式：将各$P(B_i)$与$P(A|B_i)$相乘并求和，得到$P(A)$。
####  示例
问题：某工厂的零件由甲、乙两车间生产，甲车间生产60%，合格率95%；乙车间生产40%，合格率90%。求随机抽取一件零件为合格品的概率。

解答：

- 设$B_1$为“甲车间生产”，$B_2$为“乙车间生产”，$A$为“合格品”。
- $P(B_1) = 0.6$，$P(B_2) = 0.4$。
- $P(A|B_1) = 0.95$，$P(A|B_2) = 0.90$。
- 应用全概率公式：

$$
P(A) = 0.6 \times 0.95 + 0.4 \times 0.90 = 0.57 + 0.36 = 0.93
$$

结论：随机抽取零件为合格品的概率为93%。


