
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Buffalo">
      
      
        <link rel="canonical" href="https://github.com/buffaloboyhlh/algorithmbook/machine/machinelearning/">
      
      
        <link rel="prev" href="../tutorial/">
      
      
        <link rel="next" href="../../deeplearning/math/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>机器学习教程补充 - 算法面试大集</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_2" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="算法面试大集" class="md-header__button md-logo" aria-label="算法面试大集" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            算法面试大集
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              机器学习教程补充
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="切换至夜间模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="切换至夜间模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="切换至日间模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换至日间模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../math/probability/" class="md-tabs__link">
          
  
  
    
  
  数学

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../math/" class="md-tabs__link">
          
  
  
    
  
  机器学习

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../deeplearning/math/" class="md-tabs__link">
          
  
  
    
  
  深度学习

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../reinforcement/math/" class="md-tabs__link">
          
  
  
    
  
  强化学习

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="算法面试大集" class="md-nav__button md-logo" aria-label="算法面试大集" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    算法面试大集
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    数学
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            数学
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    概率统计
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    线性代数
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    微积分与优化
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    机器学习中的数学
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    机器学习教程
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    机器学习教程补充
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    机器学习教程补充
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、线性回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、线性回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1.什么是线性回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 能够解决什么样的问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 一般表达式是什么
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 如何计算
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 过拟合、欠拟合如何解决
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 线性回归要求因变量服从正态分布？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 模型评估指标
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      二、逻辑回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、逻辑回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 什么是逻辑回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      2. 什么是Sigmoid函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      3. 损失函数是什么
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_2" class="md-nav__link">
    <span class="md-ellipsis">
      4.可以进行多分类吗？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5.逻辑回归有什么优点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6_1" class="md-nav__link">
    <span class="md-ellipsis">
      6. 逻辑回归有哪些应用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7_1" class="md-nav__link">
    <span class="md-ellipsis">
      7. 逻辑回归常用的优化方法有哪些
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8_1" class="md-nav__link">
    <span class="md-ellipsis">
      8. 逻辑斯特回归为什么要对特征进行离散化。
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9-l1" class="md-nav__link">
    <span class="md-ellipsis">
      9. 逻辑回归的目标函数中增大L1正则化会是什么结果。
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      三、决策树
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、决策树">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 决策树的基本概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 重要概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 决策树的代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    深度学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            深度学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deeplearning/math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    深度学习中的数学
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deeplearning/tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    深度学习教程
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    强化学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            强化学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../reinforcement/math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    强化学习中的数学
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../reinforcement/tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    强化学习教程
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、线性回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、线性回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1.什么是线性回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 能够解决什么样的问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 一般表达式是什么
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 如何计算
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 过拟合、欠拟合如何解决
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 线性回归要求因变量服从正态分布？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 模型评估指标
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      二、逻辑回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、逻辑回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 什么是逻辑回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      2. 什么是Sigmoid函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      3. 损失函数是什么
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_2" class="md-nav__link">
    <span class="md-ellipsis">
      4.可以进行多分类吗？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5.逻辑回归有什么优点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6_1" class="md-nav__link">
    <span class="md-ellipsis">
      6. 逻辑回归有哪些应用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7_1" class="md-nav__link">
    <span class="md-ellipsis">
      7. 逻辑回归常用的优化方法有哪些
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8_1" class="md-nav__link">
    <span class="md-ellipsis">
      8. 逻辑斯特回归为什么要对特征进行离散化。
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9-l1" class="md-nav__link">
    <span class="md-ellipsis">
      9. 逻辑回归的目标函数中增大L1正则化会是什么结果。
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      三、决策树
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、决策树">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 决策树的基本概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 重要概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 决策树的代码实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">机器学习算法<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2">一、线性回归<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="1">1.什么是线性回归<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<ul>
<li>线性：两个变量之间的关系<strong>是</strong>一次函数关系的——图象<strong>是直线</strong>，叫做线性。</li>
<li>非线性：两个变量之间的关系<strong>不是</strong>一次函数关系的——图象<strong>不是直线</strong>，叫做非线性。</li>
<li>回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算<strong>回归到真实值</strong>，这就是回归的由来。</li>
</ul>
<table>
<thead>
<tr>
<th>特性</th>
<th>一元线性回归</th>
<th>多元线性回归</th>
<th>多项式回归</th>
</tr>
</thead>
<tbody>
<tr>
<td>自变量数量</td>
<td>1个</td>
<td>2个或以上</td>
<td>1个（含高次项）</td>
</tr>
<tr>
<td>模型形式</td>
<td>线性直线</td>
<td>多维超平面</td>
<td>曲线（非线性拟合）</td>
</tr>
<tr>
<td>适用场景</td>
<td>单因素影响分析</td>
<td>多因素综合影响研究</td>
<td>非线性关系建模</td>
</tr>
<tr>
<td>关键假设</td>
<td>线性、正态误差、等方差、独立性</td>
<td>同左，外加无多重共线性</td>
<td>同线性回归，但放宽线性假设</td>
</tr>
<tr>
<td>参数估计</td>
<td>最小二乘法</td>
<td>最小二乘法或正则化方法</td>
<td>最小二乘法结合模型选择</td>
</tr>
</tbody>
</table>
<h3 id="2">2. 能够解决什么样的问题<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p>对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。</p>
<h3 id="3">3. 一般表达式是什么<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?Y=wx+b" /></p>
<p>w叫做x的系数，b叫做偏置项。</p>
<h3 id="4">4. 如何计算<a class="headerlink" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41-loss-function-mse">4.1 Loss Function--MSE<a class="headerlink" href="#41-loss-function-mse" title="Permanent link">&para;</a></h4>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?J=\frac{1}{2m}\sum^{i=1}_{m}(y^{'}-y)^2" /></p>
<p>利用<strong>梯度下降法</strong>找到最小值点，也就是最小误差，最后把 w 和 b 给求出来。</p>
<h3 id="5">5. 过拟合、欠拟合如何解决<a class="headerlink" href="#5" title="Permanent link">&para;</a></h3>
<p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h4 id="51-l2">5.1 什么是L2正则化(岭回归)<a class="headerlink" href="#51-l2" title="Permanent link">&para;</a></h4>
<p>方程：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?J=J_0+\lambda\sum_{w}w^2" /></p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?J_0" />表示上面的 loss function ，在loss function的基础上加入w参数的平方和乘以 <img alt="" src="https://latex.codecogs.com/gif.latex?\lambda" /> ，假设：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?L=\lambda({w_1}^2+{w_2}^2)" /></p>
<p>回忆以前学过的单位元的方程：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?x^2+y^2=1" /></p>
<p>正和L2正则化项一样，此时我们的任务变成在L约束下求出J取最小值的解。求解J0的过程可以画出等值线。同时L2正则化的函数L也可以在w1 w2的二维平面上画出来。如下图：</p>
<p><img alt="image" src="https://camo.githubusercontent.com/3886773758620787d7ef9b904e4ff629371dce967c12dfe672d4a77e2fefc2fb/68747470733a2f2f7778342e73696e61696d672e636e2f6c617267652f303036333044656667793167346e7339716861316e6a333038753038396161762e6a7067" /></p>
<p>L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得w1和w2无限接近于0，达到防止过拟合的问题。</p>
<h4 id="52-l2">5.2 什么场景下用L2正则化<a class="headerlink" href="#52-l2" title="Permanent link">&para;</a></h4>
<p>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归(L2), 如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</p>
<h4 id="53-l1lasso">5.3 什么是L1正则化(Lasso回归)<a class="headerlink" href="#53-l1lasso" title="Permanent link">&para;</a></h4>
<p>L1正则化与L2正则化的区别在于惩罚项的不同：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?J=J_0+\lambda(|w_1|+|w_2|)" /></p>
<p>求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图：</p>
<p><img alt="image" src="https://i-blog.csdnimg.cn/blog_migrate/1e9fc3e092f6f3cb9ddfa376f869f464.png" /></p>
<p>惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。<strong>这就说明了L1正则化容易得到稀疏矩阵。</strong></p>
<h4 id="54-l1">5.4 什么场景下使用L1正则化<a class="headerlink" href="#54-l1" title="Permanent link">&para;</a></h4>
<p><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。</p>
<h4 id="55-elasticnet">5.5 什么是ElasticNet回归<a class="headerlink" href="#55-elasticnet" title="Permanent link">&para;</a></h4>
<p><strong>ElasticNet综合了L1正则化项和L2正则化项</strong>，以下是它的公式：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?min(\frac{1}{2m}[\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]+\lambda\sum_{j=1}^{n}|\theta|)" /></p>
<h4 id="56-elasticnet">5.6  ElasticNet回归的使用场景<a class="headerlink" href="#56-elasticnet" title="Permanent link">&para;</a></h4>
<p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
<h3 id="6">6. 线性回归要求因变量服从正态分布？<a class="headerlink" href="#6" title="Permanent link">&para;</a></h3>
<p>我们假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布<span class="arithmatex">\(N(0,delta^2)\)</span>时，因变量则符合正态分布<span class="arithmatex">\(N(ax(i)+b,delta^2)\)</span>，其中预测函数<span class="arithmatex">\(y=ax(i)+b\)</span>。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 </p>
<p>在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。</p>
<h3 id="7">7. 模型评估指标<a class="headerlink" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71-mse-mean-squared-error">7.1 均方误差（MSE, Mean Squared Error）<a class="headerlink" href="#71-mse-mean-squared-error" title="Permanent link">&para;</a></h4>
<p>均方误差是预测值与真实值之差的平方的平均值。MSE越小，表示模型的预测效果越好。</p>
<div class="arithmatex">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>其中，<span class="arithmatex">\(y_i\)</span> 是真实值，<span class="arithmatex">\(\hat{y}_i\)</span> 是预测值，<span class="arithmatex">\(n\)</span> 是样本数量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre></div>
<h4 id="72-rmse-root-mean-squared-error">7.2 均方根误差（RMSE, Root Mean Squared Error）<a class="headerlink" href="#72-rmse-root-mean-squared-error" title="Permanent link">&para;</a></h4>
<p>均方根误差是均方误差的平方根，具有与原始数据相同的单位，更易于解释。</p>
<div class="arithmatex">\[
RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^
2}
\]</div>
<div class="highlight"><pre><span></span><code><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_real</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># squared=False 表示返回的是 RMSE 而不是 MSE。</span>
</code></pre></div>
<h4 id="73-mae-mean-absolute-error">7.3 平均绝对误差（MAE, Mean Absolute Error）<a class="headerlink" href="#73-mae-mean-absolute-error" title="Permanent link">&para;</a></h4>
<p>平均绝对误差是预测值与真实值之差的绝对值的平均值。MAE越小，表示模型的预测效果越好。</p>
<div class="arithmatex">\[
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]</div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre></div>
<h4 id="74-r2-coefficient-of-determination">7.4 R²（决定系数, Coefficient of Determination）<a class="headerlink" href="#74-r2-coefficient-of-determination" title="Permanent link">&para;</a></h4>
<p>R²衡量模型解释变量总变异的比例，取值范围为0到1。R²越接近1，表示模型的解释能力越强。越接近1表示模型拟合越好</p>
<div class="arithmatex">\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]</div>
<p>其中，<span class="arithmatex">\(\bar{y}\)</span> 是真实值的均值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">r2_score</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> 
</code></pre></div>
<h3 id="8">8. 代码实现<a class="headerlink" href="#8" title="Permanent link">&para;</a></h3>
<h5 id="_3">题目描述<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h5>
<p>从给定的房屋基本信息以及房屋销售信息等，建立一个回归模型预测房屋的销售价格。 数据下载请点击：<a href="https://pan.baidu.com/share/init?surl=kVdwI3d">下载</a>，密码：mfqy。</p>
<p><strong>数据说明：</strong> 数据主要包括2014年5月至2015年5月美国King County的房屋销售价格以及房屋的基本信息。 数据分为训练数据和测试数据，分别保存在kc_train.csv和kc_test.csv两个文件中。 其中训练数据主要包括10000条记录，14个字段，主要字段说明如下： 第一列“销售日期”：2014年5月到2015年5月房屋出售时的日期 第二列“销售价格”：房屋交易价格，单位为美元，是目标预测值 第三列“卧室数”：房屋中的卧室数目 第四列“浴室数”：房屋中的浴室数目 第五列“房屋面积”：房屋里的生活面积 第六列“停车面积”：停车坪的面积 第七列“楼层数”：房屋的楼层数 第八列“房屋评分”：King County房屋评分系统对房屋的总体评分 第九列“建筑面积”：除了地下室之外的房屋建筑面积 第十列“地下室面积”：地下室的面积 第十一列“建筑年份”：房屋建成的年份 第十二列“修复年份”：房屋上次修复的年份 第十三列"纬度"：房屋所在纬度 第十四列“经度”：房屋所在经度</p>
<h4 id="1_1">1. 读取数据与预处理<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;销售日期&quot;</span><span class="p">,</span> <span class="s2">&quot;销售价格&quot;</span><span class="p">,</span> <span class="s2">&quot;卧室数&quot;</span><span class="p">,</span> <span class="s2">&quot;浴室数&quot;</span><span class="p">,</span>
                <span class="s2">&quot;房屋面积&quot;</span><span class="p">,</span> <span class="s2">&quot;停车面积&quot;</span><span class="p">,</span> <span class="s2">&quot;楼层数&quot;</span><span class="p">,</span> <span class="s2">&quot;房屋评分&quot;</span><span class="p">,</span>
                <span class="s2">&quot;建筑面积&quot;</span><span class="p">,</span> <span class="s2">&quot;地下室面积&quot;</span><span class="p">,</span> <span class="s2">&quot;建筑年份&quot;</span><span class="p">,</span> <span class="s2">&quot;修复年份&quot;</span><span class="p">,</span>
                <span class="s2">&quot;纬度&quot;</span><span class="p">,</span> <span class="s2">&quot;经度&quot;</span><span class="p">]</span>

<span class="c1"># 读取数据</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;kc_train.csv&quot;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">column_names</span><span class="p">)</span>
<span class="c1"># 数据预处理</span>
<span class="n">train_data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>  <span class="c1">#查看是否有缺失值</span>

<span class="c1"># 选取特征</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[[</span><span class="s2">&quot;卧室数&quot;</span><span class="p">,</span> <span class="s2">&quot;浴室数&quot;</span><span class="p">,</span>
                <span class="s2">&quot;房屋面积&quot;</span><span class="p">,</span> <span class="s2">&quot;停车面积&quot;</span><span class="p">,</span> <span class="s2">&quot;楼层数&quot;</span><span class="p">,</span> <span class="s2">&quot;房屋评分&quot;</span><span class="p">,</span>
                <span class="s2">&quot;建筑面积&quot;</span><span class="p">,</span> <span class="s2">&quot;地下室面积&quot;</span><span class="p">,</span> <span class="s2">&quot;建筑年份&quot;</span><span class="p">,</span> <span class="s2">&quot;修复年份&quot;</span><span class="p">]]</span>
<span class="c1"># 选取目标</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;销售价格&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<h4 id="2_1">2. 特征选择<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># 统一维度</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<h4 id="3_1">3. 模型训练<a class="headerlink" href="#3_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># 模型训练</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="c1">#模型训练</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;训练RMSE：&quot;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;线性模型参数 W：&quot;</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;线性模型截距：&quot;</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div>
<h4 id="4_1">4. 模型评估<a class="headerlink" href="#4_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># 绘图进行比较</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.family&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Songti SC&#39;</span><span class="p">]</span>
<span class="n">num</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">preds</span><span class="p">[:</span><span class="n">num</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;预测值&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Y</span><span class="p">[:</span><span class="n">num</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;真实值&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h2 id="_4">二、逻辑回归<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<h3 id="1_2">1. 什么是逻辑回归<a class="headerlink" href="#1_2" title="Permanent link">&para;</a></h3>
<p>逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞, +∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。</p>
<p>也就是把Y的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<h3 id="2-sigmoid">2. 什么是Sigmoid函数<a class="headerlink" href="#2-sigmoid" title="Permanent link">&para;</a></h3>
<p>函数公式如下：</p>
<p><img alt="image" src="https://pic3.zhimg.com/v2-495e8dd0294d5986e400c866b34b4ac4_r.jpg" /></p>
<p>函数中t无论取什么值，其结果都在[0,1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。</p>
<p>好了，接下来我们把aX+b带入t中就得到了我们的逻辑回归的一般模型方程：</p>
<p><img alt="" src="https://latex.codecogs.com/gif.latex?H(a,b)=\frac{1}{1+e^{(aX+b)}}" /></p>
<p>结果P也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<h3 id="3_2">3. 损失函数是什么<a class="headerlink" href="#3_2" title="Permanent link">&para;</a></h3>
<p>逻辑回归常用交叉熵损失函数（Cross-Entropy Loss），也叫对数损失（Log Loss）。</p>
<h4 id="_5">二分类问题的交叉熵损失函数<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<p>对于二分类问题，假设样本的真实标签为 <span class="arithmatex">\(y_i \in \{0, 1\}\)</span>，模型的预测概率为 <span class="arithmatex">\(\hat{y}_i = \sigma(z_i)\)</span>，其中 <span class="arithmatex">\(z_i = \mathbf{w}^T \mathbf{x}_i + b\)</span> 是线性组合的输出，<span class="arithmatex">\(\sigma\)</span> 为 Sigmoid 函数，<span class="arithmatex">\(\mathbf{w}\)</span> 和 <span class="arithmatex">\(b\)</span> 分别是模型的权重向量和偏置项。
交叉熵损失函数定义为：</p>
<div class="arithmatex">\[
L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]</div>
<p>其中，<span class="arithmatex">\(N\)</span> 是样本数量。</p>
<p><strong>损失函数的解释</strong></p>
<ul>
<li>当 <span class="arithmatex">\(y_i = 1\)</span> 时：损失函数为 <span class="arithmatex">\(-\log(\hat{y}_i)\)</span>，预测概率 <span class="arithmatex">\(\hat{y}_i\)</span> 越接近 1，损失越小。</li>
<li>当 <span class="arithmatex">\(y_i = 0\)</span> 时：损失函数为 <span class="arithmatex">\(-\log(1 - \hat{y}_i)\)</span>，预测概率 <span class="arithmatex">\(\hat{y}_i\)</span> 越接近 0，损失越小。</li>
</ul>
<p><strong>损失函数的性质</strong></p>
<ul>
<li>非负性：损失函数的值总是非负的。</li>
<li>凸性：对于逻辑回归，损失函数是凸函数，可以通过梯度下降等优化方法找到全局最优解。</li>
<li>概率解释：损失函数本质上是最大化样本标签的预测概率。</li>
</ul>
<p><strong>梯度下降优化</strong>
为了最小化损失函数，通常使用梯度下降算法来更新参数 <span class="arithmatex">\(\mathbf{w}\)</span> 和 <span class="arithmatex">\(b\)</span>。</p>
<ul>
<li>权重 <span class="arithmatex">\(\mathbf{w}\)</span> 的梯度：</li>
</ul>
<div class="arithmatex">\[
  \frac{\partial L}{\partial \mathbf{w}} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) \mathbf{x}_i
\]</div>
<ul>
<li>偏置 <span class="arithmatex">\(b\)</span> 的梯度：</li>
</ul>
<div class="arithmatex">\[
  \frac{\partial L}{\partial b} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)
\]</div>
<p><strong>梯度下降更新规则</strong></p>
<ul>
<li>权重更新：</li>
</ul>
<div class="arithmatex">\[
  \mathbf{w} := \mathbf{w} - \alpha \cdot \frac{\partial L}{\partial \mathbf{w}}
\]</div>
<ul>
<li>偏置更新：</li>
</ul>
<div class="arithmatex">\[
  b := b - \alpha \cdot \frac{\partial L}{\partial b}
\]</div>
<p>其中，<span class="arithmatex">\(\alpha\)</span> 是学习率。</p>
<p><strong>总结</strong></p>
<p>交叉熵损失函数在逻辑回归中用于衡量模型预测概率与真实标签之间的差异，通过梯度下降算法优化模型参数，使得预测概率尽可能接近真实标签，从而提高分类的准确性。</p>
<p><a href="https://zhuanlan.zhihu.com/p/638725320">交叉损失熵</a></p>
<h3 id="4_2">4.可以进行多分类吗？<a class="headerlink" href="#4_2" title="Permanent link">&para;</a></h3>
<p>可以的，其实我们可以从二分类问题过度到多分类问题(one vs rest)，思路步骤如下：</p>
<p>1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。</p>
<p>2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。</p>
<p>3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率pi，最后我们取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。</p>
<p><img alt="image" src="https://wx2.sinaimg.cn/large/00630Defly1g4pw11fo1tj30cv0c50tj.jpg" /></p>
<p>总之还是以二分类来依次划分，并求出最大概率结果。</p>
<h3 id="5_1">5.逻辑回归有什么优点<a class="headerlink" href="#5_1" title="Permanent link">&para;</a></h3>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强，可控度高(你要给老板讲的嘛…)。</li>
<li>训练快，feature engineering之后效果赞。</li>
<li>因为结果是概率，可以做ranking model。</li>
</ul>
<h3 id="6_1">6. 逻辑回归有哪些应用<a class="headerlink" href="#6_1" title="Permanent link">&para;</a></h3>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
<h3 id="7_1">7. 逻辑回归常用的优化方法有哪些<a class="headerlink" href="#7_1" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 一阶方法<a class="headerlink" href="#71" title="Permanent link">&para;</a></h4>
<p>梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 </p>
<h4 id="72">7.2 二阶方法：牛顿法、拟牛顿法：<a class="headerlink" href="#72" title="Permanent link">&para;</a></h4>
<p>这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。</p>
<h3 id="8_1">8. 逻辑斯特回归为什么要对特征进行离散化。<a class="headerlink" href="#8_1" title="Permanent link">&para;</a></h3>
<ol>
<li>非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； </li>
<li>速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； </li>
<li>鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； </li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； </li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； </li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
<h3 id="9-l1">9. 逻辑回归的目标函数中增大L1正则化会是什么结果。<a class="headerlink" href="#9-l1" title="Permanent link">&para;</a></h3>
<p>所有的参数w都会变成0。</p>
<h3 id="10">10. 代码实现<a class="headerlink" href="#10" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span>

<span class="c1"># 加载数据集</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># 划分训练集和测试集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 数据标准化</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 训练模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h2 id="_6">三、决策树<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="31">3.1 决策树的基本概念<a class="headerlink" href="#31" title="Permanent link">&para;</a></h3>
<p>决策树是机器学习中常用的监督学习算法，用于分类和回归任务。它通过树状结构模拟决策过程，每个内部节点代表一个特征上的判断条件，每个分支代表判断结果，叶节点代表最终的预测结果。</p>
<h4 id="_7">基本结构<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<ul>
<li>根节点：树的起点，包含所有样本数据，是第一个特征判断的起点。</li>
<li>内部节点：表示对某个特征的判断条件，每个内部节点会根据特征值分裂为多个子节点。</li>
<li>分支：连接节点的线段，代表特征判断的结果。</li>
<li>叶节点：树的终点，输出最终的预测结果。</li>
</ul>
<h4 id="_8">核心原理<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<ul>
<li>特征选择：选择最优特征进行节点分裂，常用指标包括信息增益、信息增益率和基尼指数。<ul>
<li>信息熵：衡量数据集的纯度，熵值越低，纯度越高。</li>
<li>信息增益：表示特征使得类不确定性的减少程度，增益越大，特征越优。</li>
<li>信息增益率：对信息增益进行归一化，抑制对多取值属性的偏好。</li>
<li>基尼指数：衡量数据集的不纯度，基尼指数越小，纯度越高。</li>
</ul>
</li>
<li>树的生成：递归选择最优特征，分割训练数据，生成决策树。</li>
<li>树的剪枝：简化决策树，提高泛化能力，包括预剪枝和后剪枝。</li>
</ul>
<h4 id="_9">典型算法<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<ul>
<li>ID3算法：基于信息增益选择特征，生成决策树。</li>
<li>C4.5算法：改进ID3，使用信息增益率选择特征，可处理连续值和缺失值。</li>
<li>CART算法：分类与回归树，可处理分类和回归任务，使用基尼指数或平方误差选择特征。</li>
</ul>
<h4 id="_10">优缺点<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<ul>
<li>优点<ul>
<li>易于理解和解释：树形结构直观，规则清晰。</li>
<li>处理多种数据类型：可处理数值型和类别型数据。</li>
<li>不需要数据预处理：对缺失值不敏感，无需归一化。</li>
</ul>
</li>
<li>缺点<ul>
<li>容易过拟合：需通过剪枝提高泛化能力。</li>
<li>不稳定：数据微小变化可能导致树结构变化较大。</li>
</ul>
</li>
</ul>
<h4 id="_11">应用场景<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<ul>
<li>分类任务：如客户分类、疾病诊断。</li>
<li>回归任务：如房价预测、销售额预测。</li>
<li>特征选择：通过决策树选择重要特征。</li>
</ul>
<p>决策树因其直观性和可解释性，成为机器学习中的基础算法，为更复杂的集成学习算法（如随机森林、GBDT）奠定了基础。</p>
<h3 id="32">3.2 重要概念<a class="headerlink" href="#32" title="Permanent link">&para;</a></h3>
<h4 id="entropy">熵（Entropy）<a class="headerlink" href="#entropy" title="Permanent link">&para;</a></h4>
<p>衡量数据集的纯度或不确定性。熵越低，数据越纯。</p>
<div class="arithmatex">\[
H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
\]</div>
<p>数据集 S 有 c 类，第 i 类样本比例为 <span class="arithmatex">\(p_i\)</span>。</p>
<h4 id="information-gain">信息增益（Information Gain）<a class="headerlink" href="#information-gain" title="Permanent link">&para;</a></h4>
<p><strong>信息增益：</strong> 以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。</p>
<p>信息增益 = entroy(前) - entroy(后)</p>
<div class="arithmatex">\[
IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)
\]</div>
<p>数据集 S 按特征 A 的取值划分为子集 <span class="arithmatex">\(S_v\)</span>。</p>
<h4 id="information-gain-ratio">信息增益率（Information Gain Ratio）<a class="headerlink" href="#information-gain-ratio" title="Permanent link">&para;</a></h4>
<p>信息增益率是信息增益的归一化形式，解决了信息增益对多值特征的偏好问题。</p>
<div class="arithmatex">\[
IGR(S, A) = \frac{IG(S, A)}{H_A(S)}
\]</div>
<p>其中，<span class="arithmatex">\(H_A(S)\)</span> 是特征 A 的固有熵：</p>
<div class="arithmatex">\[
H_A(S) = -\sum_{v \in Values(A)} \frac{|S v|}{|S|} \log_2 \frac{|S_v|}{|S|}
\]</div>
<h4 id="gini-index">基尼指数（Gini Index）<a class="headerlink" href="#gini-index" title="Permanent link">&para;</a></h4>
<p>基尼指数衡量数据集的不纯度，值越小表示数据越纯。</p>
<div class="arithmatex">\[
Gini(S) = 1 - \sum_{i=1}^{c} p_i^2 
\]</div>
<p>数据集 S 有 c 类，第 i 类样本比例为 <span class="arithmatex">\(p_i\)</span>。</p>
<h3 id="33">3.3 决策树的代码实现<a class="headerlink" href="#33" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>
</code></pre></div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.section", "navigation.expand", "navigation.tabs", "navigation.sections", "navigation.indexes", "content.code.copy", "content.tabs.link", "content.code.annotate", "math"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="../../styles/javascripts/config.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../styles/javascripts/katex.js"></script>
      
    
  </body>
</html>